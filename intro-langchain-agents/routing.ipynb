{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9e45e81c-e16e-4c6c-b6a3-2362e5193827",
   "metadata": {},
   "source": [
    "---\n",
    "sidebar_position: 3\n",
    "keywords: [RunnableBranch, LCEL]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47436a",
   "metadata": {},
   "source": [
    "# How to route between sub-chains\n",
    "\n",
    ":::info Prerequisites\n",
    "\n",
    "This guide assumes familiarity with the following concepts:\n",
    "- [LangChain Expression Language (LCEL)](/docs/concepts/lcel)\n",
    "- [Chaining runnables](/docs/how_to/sequence/)\n",
    "- [Configuring chain parameters at runtime](/docs/how_to/configure)\n",
    "- [Prompt templates](/docs/concepts/prompt_templates)\n",
    "- [Chat Messages](/docs/concepts/messages)\n",
    "\n",
    ":::\n",
    "\n",
    "Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing can help provide structure and consistency around interactions with models by allowing you to define states and use information related to those states as context to model calls.\n",
    "\n",
    "There are two ways to perform routing:\n",
    "\n",
    "1. Conditionally return runnables from a [`RunnableLambda`](/docs/how_to/functions) (recommended)\n",
    "2. Using a `RunnableBranch` (legacy)\n",
    "\n",
    "We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about `LangChain`, `Anthropic`, or `Other`, then routes to a corresponding prompt chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c6edac",
   "metadata": {},
   "source": [
    "## Example Setup\n",
    "First, let's create a chain that will identify incoming questions as being about `LangChain`, `Anthropic`, or `Other`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a8a1967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OpenAI'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "chain = (\n",
    "    PromptTemplate.from_template(\n",
    "        \"\"\"Given the user question below, classify it as either being about `LangChain`, `OpenAI`, or `Other`.\n",
    "\n",
    "Do not respond with more than one word.\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "Classification:\"\"\"\n",
    "    )\n",
    "    | ChatOpenAI(model_name=\"gpt-4o\")\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"how do I call OpenAI?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7655555f",
   "metadata": {},
   "source": [
    "Now, let's create three sub chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d7722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in langchain. \\\n",
    "Always answer questions starting with \"As Harrison Chase told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model_name=\"gpt-4o\")\n",
    "openai_chain = PromptTemplate.from_template(\n",
    "    \"\"\"You are an expert in openai. \\\n",
    "Always answer questions starting with \"As Sam Altman told me\". \\\n",
    "Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model_name=\"gpt-4o\")\n",
    "general_chain = PromptTemplate.from_template(\n",
    "    \"\"\"Respond to the following question:\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ") | ChatOpenAI(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8d042c",
   "metadata": {},
   "source": [
    "## Using a custom function (Recommended)\n",
    "\n",
    "You can also use a custom function to route between different outputs. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "687492da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route(info):\n",
    "    if \"openai\" in info[\"topic\"].lower():\n",
    "        return openai_chain\n",
    "    elif \"langchain\" in info[\"topic\"].lower():\n",
    "        return langchain_chain\n",
    "    else:\n",
    "        return general_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02a33c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | RunnableLambda(\n",
    "    route\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2e977a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Sam Altman told me, using OpenAI begins with understanding the platform's suite of tools and APIs designed to help you integrate powerful AI capabilities into your projects. Start by signing up on the OpenAI website to get access to their API. Once you have access, explore the documentation to understand how to implement the features you need, whether it's natural language processing, generating text, or other AI functions. Tools like GPT can be integrated into applications via straightforward API calls, allowing you to customize and scale AI functionalities to meet your specific needs. Additionally, OpenAI offers resources and support to guide you through the implementation process, ensuring you can effectively harness the power of AI in your work.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 138, 'prompt_tokens': 45, 'total_tokens': 183, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-14527f13-9f00-47fb-ba8b-2c5ade281c63-0', usage_metadata={'input_tokens': 45, 'output_tokens': 138, 'total_tokens': 183, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use OpenAI?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48913dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Harrison Chase told me, using LangChain involves a few key steps:\\n\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\n\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\n\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\n\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\n\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\\n\\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\n\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\n\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", response_metadata={'id': 'msg_01H3UXAAHG4TwxJLpxwuuVU7', 'content': [ContentBlock(text=\"As Harrison Chase told me, using LangChain involves a few key steps:\\n\\n1. **Set up your environment**: Install the necessary Python packages, including the LangChain library itself, as well as any other dependencies your application might require, such as language models or other integrations.\\n\\n2. **Understand the core concepts**: LangChain revolves around a few core concepts, like Agents, Chains, and Tools. Familiarize yourself with these concepts and how they work together to build powerful language-based applications.\\n\\n3. **Identify your use case**: Determine what kind of task or application you want to build using LangChain, such as a chatbot, a question-answering system, or a document summarization tool.\\n\\n4. **Choose the appropriate components**: Based on your use case, select the right LangChain components, such as agents, chains, and tools, to build your application.\\n\\n5. **Integrate with language models**: LangChain is designed to work seamlessly with various language models, such as OpenAI's GPT-3 or Anthropic's models. Connect your chosen language model to your LangChain application.\\n\\n6. **Implement your application logic**: Use LangChain's building blocks to implement the specific functionality of your application, such as prompting the language model, processing the response, and integrating with other services or data sources.\\n\\n7. **Test and iterate**: Thoroughly test your application, gather feedback, and iterate on your design and implementation to improve its performance and user experience.\\n\\nAs Harrison Chase emphasized, LangChain provides a flexible and powerful framework for building language-based applications, making it easier to leverage the capabilities of modern language models. By following these steps, you can get started with LangChain and create innovative solutions tailored to your specific needs.\", type='text')], 'model': 'claude-3-haiku-20240307', 'role': 'assistant', 'stop_reason': 'end_turn', 'stop_sequence': None, 'type': 'message', 'usage': Usage(input_tokens=50, output_tokens=400)})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a14d0dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2 + 2 equals 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 24, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-3a9a1b2f-bc41-44f5-94fa-b6684aea6537-0', usage_metadata={'input_tokens': 24, 'output_tokens': 9, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147b827",
   "metadata": {},
   "source": [
    "## Using a RunnableBranch\n",
    "\n",
    "A `RunnableBranch` is a special type of runnable that allows you to define a set of conditions and runnables to execute based on the input. It does **not** offer anything that you can't achieve in a custom function as described above, so we recommend using a custom function instead.\n",
    "\n",
    "A `RunnableBranch` is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input. \n",
    "\n",
    "If no provided conditions match, it runs the default runnable.\n",
    "\n",
    "Here's an example of what it looks like in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a101418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"As Sam Altman told me, using OpenAI involves first understanding the specific tools and capabilities you are interested in, such as language models like GPT or image generation tools. You start by accessing OpenAI’s API platform, signing up, and obtaining an API key. Then, you can integrate the API into your application, choosing the right model, setting parameters, and crafting your input prompts carefully to generate the desired responses. It’s important to experiment with different settings and provide clear, concise prompts to get the best results. Additionally, be sure to stay informed about any updates or changes in the platform's guidelines and capabilities.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 45, 'total_tokens': 170, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-42c78202-5d96-48f8-9cbb-45edc054357c-0', usage_metadata={'input_tokens': 45, 'output_tokens': 125, 'total_tokens': 170, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableBranch\n",
    "\n",
    "branch = RunnableBranch(\n",
    "    (lambda x: \"openai\" in x[\"topic\"].lower(), openai_chain),\n",
    "    (lambda x: \"langchain\" in x[\"topic\"].lower(), langchain_chain),\n",
    "    general_chain,\n",
    ")\n",
    "full_chain = {\"topic\": chain, \"question\": lambda x: x[\"question\"]} | branch\n",
    "full_chain.invoke({\"question\": \"how do I use OpenAI?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8caf9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As Harrison Chase told me, to use LangChain, you should start by understanding the core components like chains, agents, and prompts. Begin by defining the tasks you want to accomplish, such as generating text or answering questions. You can create chains by linking together pre-built modules or custom scripts that process data and produce the desired output. To get started, explore the LangChain documentation and examples to familiarize yourself with the API and how to integrate it into your systems. Experimentation is key, so try different configurations and fine-tune them to suit your specific use case.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 115, 'prompt_tokens': 44, 'total_tokens': 159, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-d7713466-aa4d-49d1-83f5-3e1e4ccb9eb4-0', usage_metadata={'input_tokens': 44, 'output_tokens': 115, 'total_tokens': 159, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"how do I use LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26159af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2 + 2 equals 4.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 24, 'total_tokens': 33, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_50cad350e4', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad63e003-b6db-4f75-a822-310f01ce56ad-0', usage_metadata={'input_tokens': 24, 'output_tokens': 9, 'total_tokens': 33, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": \"whats 2 + 2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f589d",
   "metadata": {},
   "source": [
    "## Routing by semantic similarity\n",
    "\n",
    "One especially useful technique is to use embeddings to route a query to the most relevant prompt. Here's an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a23457d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utils.math import cosine_similarity\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. You are great at answering math questions. \\\n",
    "You are so good because you are able to break down hard problems into their component parts, \\\n",
    "answer the component parts, and then put them together to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{query}\"\"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "prompt_templates = [physics_template, math_template]\n",
    "prompt_embeddings = embeddings.embed_documents(prompt_templates)\n",
    "\n",
    "\n",
    "def prompt_router(input):\n",
    "    query_embedding = embeddings.embed_query(input[\"query\"])\n",
    "    similarity = cosine_similarity([query_embedding], prompt_embeddings)[0]\n",
    "    most_similar = prompt_templates[similarity.argmax()]\n",
    "    print(\"Using MATH\" if most_similar == math_template else \"Using PHYSICS\")\n",
    "    return PromptTemplate.from_template(most_similar)\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"query\": RunnablePassthrough()}\n",
    "    | RunnableLambda(prompt_router)\n",
    "    | ChatOpenAI(model=\"gpt-4o\")\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "664bb851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PHYSICS\n",
      "A black hole is a region in space where the gravitational pull is so intense that nothing, not even light, can escape from it. This occurs when a massive star collapses under its own gravity at the end of its life cycle, compressing its mass into a very small area. This creates a point of infinite density known as a singularity, surrounded by an event horizon, which is the boundary beyond which nothing can escape. Black holes can vary in size and are typically classified into categories such as stellar-mass black holes, which form from individual stars, and supermassive black holes, which exist at the centers of galaxies, including our own Milky Way.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What's a black hole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df34e469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MATH\n",
      "A path integral, in the context of physics and mathematics, is a concept used primarily in quantum mechanics and statistical mechanics to compute quantities over an infinite number of possible paths or configurations that a system can take. Here's a breakdown of its key components and ideas:\n",
      "\n",
      "### 1. **Context in Quantum Mechanics**:\n",
      "   - In quantum mechanics, a fundamental problem is determining the probability amplitude for a particle to move from one point to another. The path integral formulation, introduced by Richard Feynman, provides a way of doing this by integrating over all possible paths that a particle can take, between the initial and final states. \n",
      "   - This is different from classical mechanics, where a system follows a single, deterministic path that minimizes the action, according to the principle of least action.\n",
      "\n",
      "### 2. **Action and Lagrangian**:\n",
      "   - The path integral is based on the concept of action (\\(S\\)), which is a functional that assigns a real number to each path. The action is defined as the integral of the Lagrangian \\(L\\) over time:\n",
      "     \\[\n",
      "     S = \\int_{t_1}^{t_2} L(q(t), \\dot{q}(t), t) \\, dt\n",
      "     \\]\n",
      "   - Here, \\(q(t)\\) is the trajectory or path the system takes, and \\(\\dot{q}(t)\\) is the time derivative of \\(q(t)\\).\n",
      "\n",
      "### 3. **Feynman's Path Integral Formula**:\n",
      "   - In the path integral formulation, the probability amplitude \\(K\\) for a particle to go from point \\(a\\) to point \\(b\\) is calculated by summing over all paths:\n",
      "     \\[\n",
      "     K(a \\to b) = \\int \\mathcal{D}[q(t)] \\, e^{\\frac{i}{\\hbar} S[q(t)]}\n",
      "     \\]\n",
      "   - \\(\\int \\mathcal{D}[q(t)]\\) represents an integral over all possible paths \\(q(t)\\), not just those conforming to classical paths.\n",
      "\n",
      "### 4. **Applications and Importance**:\n",
      "   - **Quantum Fields**: Path integrals are extensively used in quantum field theory, where fields are analogous to particles following paths.\n",
      "   - **Statistical Mechanics**: In imaginary time, path integrals are used in statistical mechanics, providing a bridge between quantum physics and thermal fluctuations.\n",
      "   - **Mathematics**: Path integrals have connections to functional integrals in mathematics and are related to stochastic processes, such as the Wiener process.\n",
      "\n",
      "### 5. **Challenges and Approximations**:\n",
      "   - Since direct computation of path integrals is often impossible due to the infinite number of paths, various approximation methods (like perturbation theory or numerical simulations) are used.\n",
      "   - Techniques such as lattice gauge theory approximate the continuum of paths by discretizing them on a lattice.\n",
      "\n",
      "In summary, a path integral offers a powerful formulation to solve and understand complex systems in quantum physics by considering the superposition of all possible histories of a system, rather than just the classical paths dictated by deterministic laws.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(\"What's a path integral\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff40bcb3",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "You've now learned how to add routing to your composed LCEL chains.\n",
    "\n",
    "Next, check out the other how-to guides on runnables in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b7498",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
